"""
Module for benchmarking LLM-generated sorting algorithms.

Handles:
- Generating prompts for LLMs.
- Safely executing the generated code within Docker.
- Evaluating correctness and performance.
"""

import time
import json
import tempfile # For creating temporary files AND directories
import os # For file path operations
import random # Moved import to top level
from collections import defaultdict
from typing import Callable, Optional # For type hinting the callback (Removed Any)
import docker # For Docker interaction
from docker.errors import APIError, ImageNotFound # Specific Docker errors (Removed ContainerError)
from requests.exceptions import ConnectionError as DockerConnectionError # For Docker daemon connection issues
from contextlib import ExitStack # To manage multiple context managers (tempdir, container)
import tarfile # To create tar archives for copying into container
import io # To handle in-memory tar archive
import socket # For socket operations with exec_run

# Import functions from the new test suite generator module
import test_suite_generator

# Using Docker containers provides better isolation than subprocess.
# Moved import random to top level

# Constant for the generic algorithm name
GENERAL_ALGORITHM_NAME = "LLM General Sort"
# Constant for the baseline benchmark label
BASELINE_ALGORITHM_NAME = "Python sorted() Baseline"
# Constant for the filename of the docker wrapper script
DOCKER_WRAPPER_SCRIPT_NAME = "docker_exec_wrapper.py"

def generate_prompt_examples(num_examples: int = 3, max_size: int = 8, min_val: int = -10, max_val: int = 10) -> list[tuple[list[int], list[int]]]:
    """Generates small input/output examples for the LLM prompt."""
    examples = []
    # Ensure basic cases are covered
    if num_examples >= 1:
        examples.append(([], [])) # Empty list
    if num_examples >= 2:
        examples.append(([5], [5])) # Single element
    if num_examples >= 3:
        examples.append(([3, 1, 4, 1, 5, 9], [1, 1, 3, 4, 5, 9])) # Basic unsorted with duplicate

    # Add more random examples if needed
    current_examples = len(examples)
    for _ in range(max(0, num_examples - current_examples)):
        size = random.randint(2, max_size)
        input_list = [random.randint(min_val, max_val) for _ in range(size)]
        output_list = sorted(input_list)
        examples.append((input_list, output_list))

    return examples[:num_examples] # Return exactly num_examples

def create_sort_prompt(examples: Optional[list[tuple[list[int], list[int]]]] = None) -> str:
    """
    Creates a prompt to ask an LLM for an efficient general-purpose sorting algorithm,
    optionally including examples.
    """
    base_prompt = (
        "Generate a Python function named `sort_algorithm` that implements an efficient sorting algorithm "
        "suitable for general use cases (handling various data distributions like random, sorted, reversed, duplicates, etc.).\n"
        "The function MUST take a list of numbers as input and return a new sorted list.\n"
        "IMPORTANT: The function MUST NOT use the built-in sorted() function or the .sort() method.\n"
        "IMPORTANT: The function MUST NOT print anything to standard output. It should ONLY return the sorted list."
    )

    if examples:
        example_str = "\n\nHere are some examples of how the function should behave:\n"
        for input_list, output_list in examples:
            example_str += f"Input: {input_list}\nOutput: {output_list}\n\n"
        return base_prompt + example_str.strip() # Add examples and remove trailing newline
    else:
        return base_prompt

# Test suite generation/loading functions are now in test_suite_generator.py

# --- Evaluation Logic ---

def evaluate_algorithm(generated_code: str, categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None) -> dict:
    """
    Evaluates the generated sorting algorithm code using provided test cases, optionally reporting progress.

    Args:
        generated_code: The Python code string generated by the LLM.
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates.
                           The dictionary passed to the callback might contain keys like:
                           'current_case', 'total_cases', 'category', 'status',
                           'input_snippet', 'output_snippet', 'error'.

    Returns:
        A dictionary containing evaluation results:
        - 'correctness': Overall correctness percentage.
        - 'avg_time_ms': Overall average LLM time (ms) for correct runs.
        - 'baseline_avg_time_ms': Overall average baseline time (ms).
        - 'performance_details': A dict mapping category names to {'correctness': float, 'avg_time_ms': float, 'baseline_avg_time_ms': float, 'count': int}.
        - 'error': Error message if execution or validation fails.
    """
    results = {
        'correctness': 0.0,
        'avg_time_ms': None,
        'baseline_avg_time_ms': None,
        'performance_details': {}, # To store per-category results
        'error': None
    }

    if not categorized_test_cases:
        results['error'] = "No test cases provided for evaluation."
        return results

    # Aggregators for overall results
    overall_correct_count = 0
    overall_llm_time = 0
    overall_baseline_time = 0
    overall_total_cases = 0
    overall_llm_runs_timed = 0 # Count only successful, correct LLM runs for averaging

    # Aggregators for per-category results
    category_results = defaultdict(lambda: {'correct_count': 0, 'llm_time': 0, 'baseline_time': 0, 'case_count': 0, 'llm_runs_timed': 0})

    # Calculate total cases for progress reporting
    total_overall_cases_calculated = sum(len(cases) for cases in categorized_test_cases.values())
    current_overall_case_num = 0
    # Define a timeout for each *individual execution* within the container
    EXEC_TIMEOUT = 120.0 # Timeout in seconds for container.exec_run
    # Define the Docker image to use
    DOCKER_IMAGE = "python:3.10-slim" # Or choose another appropriate Python image
    # Define container resource limits (adjust as needed)
    CONTAINER_MEM_LIMIT = "256m" # e.g., 256 MB memory limit
    CONTAINER_CPU_SHARES = 512 # Relative CPU weight (default 1024)

    # Initialize Docker client
    docker_client = None
    try:
        docker_client = docker.from_env(timeout=10)
        docker_client.ping()
        print("Successfully connected to Docker daemon.")
        # Ensure image exists
        try:
            docker_client.images.get(DOCKER_IMAGE)
        except ImageNotFound:
            print(f"Pulling Docker image: {DOCKER_IMAGE}...")
            docker_client.images.pull(DOCKER_IMAGE)
        print(f"Using Docker image: {DOCKER_IMAGE}")
    except (DockerConnectionError, APIError, Exception) as e:
        results['error'] = f"Docker initialization failed: {e}. Is Docker running?"
        print(f"Error: {results['error']}")
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results

    # Construct the absolute path to the Docker wrapper script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    wrapper_script_path = os.path.join(script_dir, DOCKER_WRAPPER_SCRIPT_NAME)

    # Read the Docker execution wrapper script content
    try:
        print(f"Attempting to read wrapper script from: {wrapper_script_path}") # Debug print
        with open(wrapper_script_path, 'r', encoding='utf-8') as f:
            exec_wrapper_code = f.read()
        print(f"Successfully read wrapper script: {DOCKER_WRAPPER_SCRIPT_NAME}") # Debug print
    except FileNotFoundError:
        results['error'] = f"Critical Error: Docker wrapper script '{DOCKER_WRAPPER_SCRIPT_NAME}' not found at expected path: {wrapper_script_path}."
        print(results['error'])
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results
    except Exception as e:
        results['error'] = f"Critical Error: Failed to read Docker wrapper script '{DOCKER_WRAPPER_SCRIPT_NAME}' from path '{wrapper_script_path}': {e}"
        print(results['error'])
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results


    container = None
    # Use ExitStack for robust cleanup of tempdir and container
    with ExitStack() as stack:
        try:
           # Create TemporaryDirectory for the LLM code and runner script
           temp_dir = stack.enter_context(tempfile.TemporaryDirectory())
           llm_code_filename = "llm_sort.py"
           # Use the constant for the wrapper script filename on the host
           runner_script_filename_host = DOCKER_WRAPPER_SCRIPT_NAME
           llm_code_path_host = os.path.join(temp_dir, llm_code_filename)
           # The runner script is read from its fixed location, but we'll write its content
           # to the temp dir for copying into the container, using a consistent name inside.
           runner_script_filename_cont = "exec_runner.py" # Name inside container
           runner_script_path_host = os.path.join(temp_dir, runner_script_filename_cont) # Path on host temp dir

           sandbox_dir = "/sandbox" # Mount point inside container
           llm_code_path_cont = f"{sandbox_dir}/{llm_code_filename}"
           runner_script_path_cont = f"{sandbox_dir}/{runner_script_filename_cont}" # Container path for runner

           # --- DEBUG: Verify host files before container start ---
           print(f"DEBUG: Host temporary directory: {temp_dir}")
           print(f"DEBUG: Host LLM code path: {llm_code_path_host}")
           print(f"DEBUG: Host runner script path (in temp): {runner_script_path_host}")
           # --- END DEBUG ---

           # Write the generated code and the wrapper script (read earlier) to the host temp directory
           with open(llm_code_path_host, 'w', encoding='utf-8') as f_llm_script:
               f_llm_script.write(generated_code)
           with open(runner_script_path_host, 'w', encoding='utf-8') as f_runner_script:
               f_runner_script.write(exec_wrapper_code) # Write the content read from docker_exec_wrapper.py

           # --- DEBUG: Check if files exist on host before mount ---
           llm_exists = os.path.exists(llm_code_path_host)
           runner_exists = os.path.exists(runner_script_path_host)
           print(f"DEBUG: Host LLM code exists before run?: {llm_exists}")
           print(f"DEBUG: Host runner script exists before run?: {runner_exists}")
           if not llm_exists or not runner_exists:
                print("ERROR: Script files not found on host before starting container!")
                # Consider raising an error here if needed
           # --- END DEBUG ---

           # Start the container once, keep it running
           print("Starting persistent Docker container...")
           container = docker_client.containers.run(
               image=DOCKER_IMAGE,
               command=["sleep", "infinity"], # Keep container alive
               # --- Volume mount removed, will copy files instead ---
               # volumes={temp_dir: {'bind': sandbox_dir}},
               working_dir=sandbox_dir, # Still useful for exec_run context
               mem_limit=CONTAINER_MEM_LIMIT,
               cpu_shares=CONTAINER_CPU_SHARES,
                detach=True,
                auto_remove=False, # We need to manage removal manually after loop
                # Security options
                # read_only=True, # Root filesystem read-only (volume is separate)
               # network_mode='none', # Disable networking
           )
           # Ensure container is stopped and removed at the end
           stack.callback(lambda c: (c.stop(timeout=5), c.remove(force=True)), container)
           print(f"Container {container.short_id} started.")

           # --- Create sandbox directory inside container ---
           # Although working_dir is set, explicitly create it for clarity and put_archive target
           print(f"Creating {sandbox_dir} inside container...")
           container.exec_run(cmd=f"mkdir -p {sandbox_dir}")

           # --- Prepare and copy files into the container ---
           # Use the container-internal runner script name in the log message
           print(f"Copying {llm_code_filename} and {runner_script_filename_cont} to container:{sandbox_dir}...")
           # Create an in-memory tar archive
           tar_stream = io.BytesIO()
           with tarfile.open(fileobj=tar_stream, mode='w') as tar:
               # Add llm_sort.py
               tar.add(llm_code_path_host, arcname=llm_code_filename)
               # Add the runner script using its container-internal name
               tar.add(runner_script_path_host, arcname=runner_script_filename_cont)

           tar_stream.seek(0) # Rewind the stream
           # Copy the archive to the container
           container.put_archive(path=sandbox_dir, data=tar_stream)
           print("Files copied.")

           # Optional: Short delay after copy might still be beneficial
           # time.sleep(1)

           # Iterate through each category and its test cases
           for category, test_cases_in_category in categorized_test_cases.items():
               print(f"  Evaluating category: {category} ({len(test_cases_in_category)} cases)")
               cat_stats = category_results[category] # Get stats dict for this category
               num_cases_in_category = len(test_cases_in_category)

               for i, test_case in enumerate(test_cases_in_category):
                   current_overall_case_num += 1
                   overall_total_cases += 1 # Increment overall counter here
                   cat_stats['case_count'] += 1

                   # --- Prepare data for callback ---
                   progress_data = {
                       'current_case': current_overall_case_num,
                       'total_cases': total_overall_cases_calculated,
                       'category': category,
                       'category_case_num': i + 1,
                       'category_total_cases': num_cases_in_category,
                       'status': 'Running',
                       'input_snippet': repr(test_case[:10]) + ('...' if len(test_case) > 10 else ''),
                       'output_snippet': None,
                       'error': None
                   }
                   if progress_callback:
                       progress_callback(progress_data) # Report start of case processing

                   # Prepare inputs
                   baseline_input = list(test_case) # For baseline timing
                   expected_output = sorted(test_case) # Ground truth
                   input_json = json.dumps(test_case)

                   # --- DEBUG: List directory contents before executing ---
                   try:
                       ls_cmd = ["ls", "-l", sandbox_dir]
                       ls_result = container.exec_run(cmd=ls_cmd, stream=False, demux=False)
                       ls_output = "(no output)"
                       if ls_result.output:
                           ls_output = ls_result.output.decode('utf-8', errors='replace').strip()
                       print(f"DEBUG: Contents of {sandbox_dir} before exec: Exit={ls_result.exit_code}\n{ls_output}")
                   except Exception as ls_e:
                       print(f"DEBUG: Error listing {sandbox_dir}: {ls_e}")
                   # --- END DEBUG ---

                   # --- Execute code inside the running container using exec_run ---
                   actual_output = None
                   llm_error_str = None
                   current_llm_time_ms = None # Time reported by the container script
                   is_correct = False

                   host_exec_start_time = time.perf_counter() # Host-side timing for exec_run call itself

                   try:
                       # Command to execute the runner script file directly inside the container
                       # Use the container-internal runner script path
                       exec_command = ["python", runner_script_path_cont] # e.g., ["python", "/sandbox/exec_runner.py"]

                       # Use socket=True for reliable stdin/stdout handling
                       exec_id = docker_client.api.exec_create(
                           container.id,
                           cmd=exec_command,
                           stdin=True,
                           stdout=True,
                           stderr=True, # Still capture stderr, wrapper combines it
                           workdir=sandbox_dir,
                       )
                       _sock = docker_client.api.exec_start(exec_id, socket=True)
                       _sock_low_level = _sock._sock # Get the underlying socket

                       # Define and set socket timeout
                       SOCKET_TIMEOUT = 120.0 # seconds - Adjust if needed for very large cases
                       _sock_low_level.settimeout(SOCKET_TIMEOUT)

                       output_bytes = b""
                       parsed_result = None

                       try:
                           # Write input JSON to stdin stream using sendall
                           input_bytes = input_json.encode('utf-8')
                           _sock_low_level.sendall(input_bytes)
                           # IMPORTANT: Close the write half to signal EOF to the container's stdin.read()
                           _sock_low_level.shutdown(socket.SHUT_WR)

                           # Read Docker stream protocol (header + payload)
                           output_bytes = b"" # Initialize to empty
                           try:
                               import struct # Needed for unpacking header

                               while True:
                                   # Read the 8-byte header
                                   header = _sock_low_level.recv(8)
                                   if not header:
                                       break # Connection closed normally

                                   if len(header) < 8:
                                       llm_error_str = f"Socket error: Received incomplete stream header (got {len(header)} bytes)."
                                       output_bytes = b"" # Discard potentially corrupt data
                                       break

                                   # Unpack the header (stream_type: 1 byte, size: 4 bytes big-endian)
                                   stream_type = header[0]
                                   payload_size = struct.unpack('>I', header[4:])[0]

                                   # Read the payload
                                   payload = b""
                                   remaining_size = payload_size
                                   while remaining_size > 0:
                                       chunk = _sock_low_level.recv(min(remaining_size, 4096))
                                       if not chunk:
                                           llm_error_str = f"Socket error: Connection closed while reading payload (expected {payload_size} bytes, got {len(payload)})."
                                           output_bytes = b"" # Discard partial payload
                                           break # Exit inner loop
                                       payload += chunk
                                       remaining_size -= len(chunk)

                                   if llm_error_str: # Check if inner loop broke due to error
                                       break # Exit outer loop

                                   # We only care about the stdout stream (type 1) for the JSON result
                                   if stream_type == 1:
                                       output_bytes = payload # Store the stdout payload
                                       # Assume the first stdout payload is the complete JSON result
                                       # and break the loop. If more output is expected, adjust logic.
                                       break
                                   elif stream_type == 2:
                                       # Log stderr for debugging if needed
                                       print(f"DEBUG: Received stderr stream from container: {payload.decode('utf-8', errors='replace')}")
                                       # Continue reading in case stdout follows stderr
                                   else:
                                       # Ignore other stream types (e.g., stdin type 0)
                                       print(f"DEBUG: Ignoring stream type {stream_type} with size {payload_size}")
                                       # Continue reading

                           except socket.timeout:
                               llm_error_str = f"Socket operation timed out after {SOCKET_TIMEOUT} seconds (LLM code likely too slow or hung)."
                           except (socket.error, BrokenPipeError, OSError) as sock_err:
                               llm_error_str = f"Socket error during exec stream read: {sock_err}"
                           except struct.error as unpack_err:
                               llm_error_str = f"Error unpacking Docker stream header: {unpack_err}. Header bytes: {repr(header)}"
                           except Exception as stream_err:
                               llm_error_str = f"Error during socket stream I/O: {stream_err}"
                           finally:
                               # Ensure socket is closed
                               if _sock_low_level:
                                   _sock_low_level.close()
                               if _sock: # Close the high-level wrapper too
                                   _sock.close()

                       host_exec_end_time = time.perf_counter() # Timing remains similar

                       # Check exec exit code *after* reading output
                       exec_inspect = docker_client.api.exec_inspect(exec_id)
                       exit_code = exec_inspect.get('ExitCode')

                       # Decode and parse output bytes (similar logic as before)
                       if llm_error_str is None: # Only proceed if no socket error
                           output_str = "" # Initialize output_str
                           try:
                               output_str = output_bytes.decode('utf-8', errors='replace').strip()
                           except Exception as decode_err:
                               raw_bytes_repr = repr(output_bytes[:200])
                               llm_error_str = (f"Host failed to decode exec output: {decode_err}. "
                                                f"Raw bytes: {raw_bytes_repr}")

                           if output_str and llm_error_str is None:
                               try:
                                   # Find the last line that looks like JSON
                                   last_line = output_str.splitlines()[-1]
                                   parsed_result = json.loads(last_line)
                               except (json.JSONDecodeError, IndexError) as json_e:
                                   llm_error_str = f"Failed to decode JSON result from exec output: {json_e}. Full output:\n---\n{output_str[:1000]}\n---"
                               except Exception as parse_e:
                                   llm_error_str = f"Unexpected error parsing exec output: {parse_e}. Full output:\n---\n{output_str[:1000]}\n---"

                       # --- Process the parsed result (logic remains largely the same) ---
                       if llm_error_str: # If host-level parsing or socket error occurred
                           pass # Error already set
                       elif exit_code is None:
                           llm_error_str = f"Exec did not return an exit code (inspect result: {exec_inspect})."
                       elif exit_code != 0:
                           llm_error_str = f"Exec wrapper exited with code {exit_code}."
                           # Append JSON error if available, otherwise append raw output
                           if parsed_result and parsed_result.get('error'):
                               llm_error_str += f" Internal error:\n---\n{parsed_result['error']}\n---"
                           elif output_str:
                               llm_error_str += f" Raw output:\n---\n{output_str[:1000]}\n---"
                       elif parsed_result is None:
                           # Should not happen if exit code is 0 and no parsing error occurred, but check defensively
                           llm_error_str = "Exec wrapper exited code 0 but host failed to parse JSON result."
                           if output_str:
                               output_snippet = output_str[:1000]
                               llm_error_str += f" Raw output:\n---\n{output_snippet}\n---"
                       elif parsed_result.get('error'):
                           # Exit code 0, but the script internally caught an error
                           internal_error = parsed_result['error']
                           llm_error_str = f"Exec wrapper reported internal error:\n---\n{internal_error}\n---"
                       else:
                           # --- Success Case ---
                           actual_output = parsed_result.get('output')
                           current_llm_time_ms = parsed_result.get('exec_time_ms') # Use time measured inside container

                           if actual_output == expected_output:
                               is_correct = True
                               overall_correct_count += 1
                               cat_stats['correct_count'] += 1
                               if current_llm_time_ms is not None:
                                   # Convert ms to seconds for aggregation
                                   time_sec = current_llm_time_ms / 1000.0
                                   overall_llm_time += time_sec
                                   cat_stats['llm_time'] += time_sec
                                   overall_llm_runs_timed += 1
                                   cat_stats['llm_runs_timed'] += 1
                               progress_data['status'] = 'Correct'
                           else:
                               # Log incorrect sort
                               actual_repr = repr(actual_output)
                               if isinstance(actual_output, list) and len(actual_output) > 20:
                                   actual_repr = repr(actual_output[:20]) + '...'
                               expected_repr = repr(expected_output)
                               if len(expected_output) > 20:
                                   expected_repr = repr(expected_output[:20]) + '...'
                               test_repr = repr(test_case)
                               if len(test_case) > 20:
                                   test_repr = repr(test_case[:20]) + '...'
                               print(f"    Incorrect sort: Input={test_repr}, Expected={expected_repr}, Got={actual_repr}")
                               progress_data['status'] = 'Incorrect'
                               progress_data['output_snippet'] = actual_repr
                                 # Optionally capture the incorrect output in llm_error_str for reporting?
                                 # llm_error_str = f"Incorrect output. Expected: {expected_repr}, Got: {actual_repr}"

                   # Catch host-side errors during exec_run call itself
                   except (APIError, DockerConnectionError) as docker_exec_err:
                       llm_error_str = f"Docker API/Connection error during exec_run: {docker_exec_err}"
                       # This might be fatal for subsequent calls, consider aborting? For now, report per case.
                   except Exception as host_exec_e:
                       llm_error_str = f"Host error during container exec_run setup or call: {host_exec_e}"


                   # --- Handle LLM Run Outcome for this case ---
                   if llm_error_str:
                       test_repr = repr(test_case[:20]) + '...' if len(test_case) > 20 else repr(test_case)
                       print(f"    Error during LLM sort execution: Input={test_repr}, Error={llm_error_str}")
                       if progress_callback:
                           progress_data['status'] = 'Error'
                           progress_data['error'] = llm_error_str
                           progress_callback(progress_data)
                   elif progress_callback: # If no error, update progress (Correct/Incorrect status set above)
                       progress_callback(progress_data)


                   # --- Time Python's built-in sorted() ---
                   baseline_start_time = time.perf_counter()
                   current_baseline_time = None
                   try:
                       _ = sorted(baseline_input) # Execute baseline sort
                       baseline_end_time = time.perf_counter()
                       current_baseline_time = baseline_end_time - baseline_start_time
                       overall_baseline_time += current_baseline_time
                       cat_stats['baseline_time'] += current_baseline_time
                   except Exception as e:
                       test_repr = repr(test_case)
                       if len(test_case) > 20:
                           test_repr = repr(test_case[:20]) + '...'
                       print(f"    Error during baseline sort execution: Input={test_repr}, Error={e}")
                       # Decide how to handle baseline errors (e.g., skip timing for this case?)

               # --- End of loop over test cases within a category ---
           # --- End of loop over categories ---

        # Catch errors during the initial container setup phase
        except (APIError, DockerConnectionError) as docker_setup_err:
            llm_error_str = f"Docker API/Connection error during container setup: {docker_setup_err}"
            results['error'] = llm_error_str
            print(f"Aborting evaluation due to Docker setup error: {llm_error_str}")
            if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
            # ExitStack will handle cleanup if container was partially started
            return results
        except Exception as setup_exec_e:
            llm_error_str = f"Error setting up container or temporary directory: {setup_exec_e}"
            results['error'] = llm_error_str
            print(f"Aborting evaluation due to setup error: {llm_error_str}")
            if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
            # ExitStack will handle cleanup
            return results
        finally:
            # ExitStack automatically handles stopping/removing the container and deleting the temp dir
            if container:
                 print(f"Container {container.short_id} stopped and removed.")
            else:
                 print("No container was started or setup failed.")


    # --- Calculate Final Results ---
    # (Error handling for overall execution moved outside the loop)
    # Check if any fundamental error occurred before processing cases (e.g., invalid generated_code structure)
    # This part is less relevant now as syntax errors are caught per-case by subprocess

    # Overall results
        if overall_total_cases > 0:
            results['correctness'] = (overall_correct_count / overall_total_cases) * 100
            results['baseline_avg_time_ms'] = (overall_baseline_time / overall_total_cases) * 1000
        if overall_llm_runs_timed > 0: # Average LLM time only over correctly sorted cases
            results['avg_time_ms'] = (overall_llm_time / overall_llm_runs_timed) * 1000

       # Per-category results
        for category, stats in category_results.items():
           cat_correctness = 0.0
           if stats['case_count'] > 0:
               cat_correctness = (stats['correct_count'] / stats['case_count']) * 100

           cat_avg_llm_time = None
           if stats['llm_runs_timed'] > 0:
               cat_avg_llm_time = (stats['llm_time'] / stats['llm_runs_timed']) * 1000

           cat_avg_baseline_time = None
           if stats['case_count'] > 0:
               cat_avg_baseline_time = (stats['baseline_time'] / stats['case_count']) * 1000

           results['performance_details'][category] = {
               'correctness': cat_correctness,
               'avg_time_ms': cat_avg_llm_time, # Note: Includes container exec overhead
                'baseline_avg_time_ms': cat_avg_baseline_time,
                'count': stats['case_count']
            }

    # No top-level try-except for SyntaxError needed anymore, handled per case.
    # General errors during setup might still occur but are less likely.

    return results


# Note: algorithm_name parameter is removed. We use the constant GENERAL_ALGORITHM_NAME internally.
# Now accepts pre-generated code.
def run_single_benchmark(llm_name: str, generated_code: str, categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None) -> dict:
    """
    Runs the evaluation part of a benchmark for a single LLM, using pre-generated code
    and provided test cases, optionally reporting progress.

    Args:
        llm_name: The identifier for the LLM used (for reporting).
        generated_code: The Python code string generated by the LLM.
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates during evaluation.

    Returns:
        A dictionary containing benchmark evaluation results.
    """
    algorithm_name = GENERAL_ALGORITHM_NAME # Use the constant label
    print(f"Evaluating benchmark for {llm_name} ({algorithm_name}) using provided code...")

    # Code generation is now done *before* calling this function.
    # The initial progress callback indicating evaluation start is also handled before this call.

    # --- Run Evaluation ---
    evaluation = evaluate_algorithm(
        generated_code=generated_code, # Use the passed-in code
        categorized_test_cases=categorized_test_cases,
        progress_callback=progress_callback # Pass callback for evaluation steps
    )

    # --- Callback for completion of evaluation ---
    final_status = 'Completed' if not evaluation.get('error') else 'Error'
    if progress_callback:
        progress_callback({
            'status': final_status, 'category': 'Finished',
            'error': evaluation.get('error')
        })

    return {
        'llm': llm_name,
        'algorithm': algorithm_name,
        'correctness': evaluation.get('correctness'),
        'avg_time_ms': evaluation.get('avg_time_ms'),
        'baseline_avg_time_ms': evaluation.get('baseline_avg_time_ms'),
        'performance_details': evaluation.get('performance_details'),
        'error': evaluation.get('error'),
        # 'generated_code' is no longer added here, it's handled in app.py
    }

# Note: algorithm_name parameter removed, using constant label BASELINE_ALGORITHM_NAME instead.
def run_python_sorted_benchmark(categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None, python_sorted_identifier: str = "Python sorted()") -> dict:
    """
    Runs a benchmark using Python's built-in sorted() function as a baseline,
    using provided test cases and optionally reporting progress.

    Args:
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates.

    Returns:
        A dictionary containing benchmark results.
    """
    algorithm_name = BASELINE_ALGORITHM_NAME # Use the constant label
    print(f"Running {algorithm_name} benchmark...")
    results = {
        'llm': python_sorted_identifier, # Use the passed identifier (e.g., from app.py)
        'algorithm': algorithm_name, # Use the constant label
        'correctness': 100.0, # sorted() is assumed correct
        'avg_time_ms': None, # This is the baseline itself
        'baseline_avg_time_ms': None, # Will be calculated
        'performance_details': {}, # To store per-category results
        'error': None,
        'generated_code': "N/A - Python sorted()"
    }

    if not categorized_test_cases:
        results['error'] = "No test cases provided for baseline benchmark."
        return results

    # Aggregators for overall results
    overall_baseline_time = 0
    overall_total_cases = 0

    # Aggregators for per-category results
    category_results = defaultdict(lambda: {'baseline_time': 0, 'case_count': 0})

    try:
        # Calculate total cases for progress reporting
        total_overall_cases_calculated = sum(len(cases) for cases in categorized_test_cases.values())
        current_overall_case_num = 0

        # Iterate through each category and its test cases
        for category, test_cases_in_category in categorized_test_cases.items():
            print(f"  Benchmarking category: {category} ({len(test_cases_in_category)} cases)")
            cat_stats = category_results[category] # Get stats dict for this category
            num_cases_in_category = len(test_cases_in_category)

            for i, test_case in enumerate(test_cases_in_category):
                current_overall_case_num += 1
                overall_total_cases += 1 # Increment the overall counter
                cat_stats['case_count'] += 1 # Increment here

                # --- Prepare data for callback ---
                progress_data = {
                    'current_case': current_overall_case_num,
                    'total_cases': total_overall_cases_calculated,
                    'category': category,
                   'category_case_num': i + 1,
                   'category_total_cases': num_cases_in_category,
                   'status': 'Running',
                   'input_snippet': repr(test_case[:10]) + ('...' if len(test_case) > 10 else ''),
                   # Calculate the expected output snippet for baseline
                   'output_snippet': repr(sorted(test_case)[:10]) + ('...' if len(test_case) > 10 else ''),
                   'error': None
               }
               # Report start of case processing (optional for baseline, but consistent)
                # if progress_callback: progress_callback(progress_data)

                baseline_input = list(test_case)
                start_time = time.perf_counter()
                _ = sorted(baseline_input) # Execute and time sorted()
                end_time = time.perf_counter()
                current_baseline_time = end_time - start_time

                overall_baseline_time += current_baseline_time
                cat_stats['baseline_time'] += current_baseline_time

                # Report completion of case for baseline
                if progress_callback:
                    progress_data['status'] = 'Correct (Baseline)' # Baseline is assumed correct
                    progress_callback(progress_data)


        # --- Calculate Final Results ---

        # Overall results
        if overall_total_cases > 0:
            overall_avg_time = (overall_baseline_time / overall_total_cases) * 1000
            # Since this *is* the baseline, set both avg_time and baseline_avg_time
            results['avg_time_ms'] = overall_avg_time
            results['baseline_avg_time_ms'] = overall_avg_time
        else:
             results['avg_time_ms'] = 0.0
             results['baseline_avg_time_ms'] = 0.0

        # Per-category results
        for category, stats in category_results.items():
           cat_avg_baseline_time = None
           if stats['case_count'] > 0:
               cat_avg_baseline_time = (stats['baseline_time'] / stats['case_count']) * 1000
           results['performance_details'][category] = {
               'correctness': 100.0, # Assumed correct
               'avg_time_ms': cat_avg_baseline_time, # This is the baseline time for this category
                'baseline_avg_time_ms': cat_avg_baseline_time,
                'count': stats['case_count']
            }

    except Exception as e:
        results['error'] = f"Error during Python sorted() execution: {e}"
        results['correctness'] = None # Mark correctness as unknown if execution fails
        # Ensure performance_details is still present, even if empty
        results['performance_details'] = results.get('performance_details', {})

    return results


# Note: The __main__ block below is for standalone testing/example usage of benchmark.py.
# Test suite generation is now handled by test_suite_generator.py.

if __name__ == '__main__':
    # Import argparse here as it's only needed for CLI execution
    import argparse
    # Import the constant needed for the example run, if running standalone
    # Define a local constant for the example run if app isn't available
    LOCAL_PYTHON_SORTED_BENCHMARK_ID = "Python sorted() [Standalone]"

    parser = argparse.ArgumentParser(description="Benchmark Execution Examples")
    # Use the default from the generator module
    parser.add_argument('--suite-file', default=test_suite_generator.DEFAULT_TEST_SUITE_FILE, help="Specify the test suite JSON file path")

    args = parser.parse_args()

    # Example of running benchmarks using a loaded suite
    print("Running example benchmarks with loaded suite...")
    try:
        # Load the test suite using the function from the generator module
        test_suite = test_suite_generator.load_test_suite(args.suite_file)

        # Example usage - Load suite first, then run benchmarks
        print("\nRunning example benchmarks with loaded suite...")

        # Run baseline benchmark
        print("\nRunning baseline benchmark example...")
        # Pass the local identifier for standalone runs
        result_baseline = run_python_sorted_benchmark(
            categorized_test_cases=test_suite,
            python_sorted_identifier=LOCAL_PYTHON_SORTED_BENCHMARK_ID
        )
        print("\nPython sorted() Benchmark Result:\n", json.dumps(result_baseline, indent=2))

        # --- Run example with fixed Merge Sort code ---
        print("\nRunning example benchmark with fixed Merge Sort code...")

        EXAMPLE_MERGE_SORT_CODE = """
import sys
from typing import List, TypeVar

# Increase recursion depth limit for potentially deep recursion with large lists
try:
    sys.setrecursionlimit(2000)
except Exception:
    pass

T = TypeVar('T')

def sort_algorithm(arr: List[T]) -> List[T]:
    # Base Case
    if len(arr) <= 1:
        return arr[:]

    # Divide
    mid = len(arr) // 2
    left_half = arr[:mid]
    right_half = arr[mid:]

    # Conquer
    sorted_left = sort_algorithm(left_half)
    sorted_right = sort_algorithm(right_half)

    # Combine (Merge)
    merged = []
    left_idx, right_idx = 0, 0
    while left_idx < len(sorted_left) and right_idx < len(sorted_right):
        if sorted_left[left_idx] <= sorted_right[right_idx]:
            merged.append(sorted_left[left_idx])
            left_idx += 1
        else:
            merged.append(sorted_right[right_idx])
            right_idx += 1

    # Append Remaining
    while left_idx < len(sorted_left):
        merged.append(sorted_left[left_idx])
        left_idx += 1
    while right_idx < len(sorted_right):
        merged.append(sorted_right[right_idx])
        right_idx += 1

    return merged
"""
            # Run the benchmark using the fixed code
        result_example = run_single_benchmark(
                llm_name="Example Merge Sort", # Use a descriptive name
                generated_code=EXAMPLE_MERGE_SORT_CODE,
                categorized_test_cases=test_suite
                # Add progress_callback=print if you want to see progress updates
            )
        print("\nExample Merge Sort Benchmark Result:\n", json.dumps(result_example, indent=2))


    except FileNotFoundError:
            print(f"Test suite file '{args.suite_file}' not found. Generate it first using: python test_suite_generator.py --generate-suite")
    except Exception as e:
            print(f"An error occurred during example benchmark run: {e}")
