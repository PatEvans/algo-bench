"""
Module for benchmarking LLM-generated sorting algorithms.

Handles:
- Generating prompts for LLMs.
- Safely executing the generated code within Docker.
- Evaluating correctness and performance.
"""

import time
import json
import tempfile # For creating temporary files AND directories
import os # For file path operations
import random # Moved import to top level
from collections import defaultdict
from typing import Callable, Optional # For type hinting the callback (Removed Any)
import docker # For Docker interaction
from docker.errors import APIError, ImageNotFound # Specific Docker errors (Removed ContainerError)
from requests.exceptions import ConnectionError as DockerConnectionError # For Docker daemon connection issues
from contextlib import ExitStack # To manage multiple context managers (tempdir, container)
import tarfile # To create tar archives for copying into container
import io # To handle in-memory tar archive
# Socket import no longer needed here as we don't interact with exec stream directly
# import socket

# Import functions from the new test suite generator module
import test_suite_generator

# Using Docker containers provides better isolation than subprocess.
# Moved import random to top level

# Constant for the generic algorithm name
GENERAL_ALGORITHM_NAME = "LLM General Sort"
# Constant for the baseline benchmark label
BASELINE_ALGORITHM_NAME = "Python sorted() Baseline"
# Constant for the filename of the docker wrapper script
DOCKER_WRAPPER_SCRIPT_NAME = "docker_exec_wrapper.py"
# Constant for the filename of the test suite data inside the container
TEST_SUITE_DATA_FILENAME = "test_suite_data.json"


def generate_prompt_examples(num_examples: int = 3, max_size: int = 8, min_val: int = -10, max_val: int = 10) -> list[tuple[list[int], list[int]]]:
    """Generates small input/output examples for the LLM prompt."""
    examples = []
    # Ensure basic cases are covered
    if num_examples >= 1:
        examples.append(([], [])) # Empty list
    if num_examples >= 2:
        examples.append(([5], [5])) # Single element
    if num_examples >= 3:
        examples.append(([3, 1, 4, 1, 5, 9], [1, 1, 3, 4, 5, 9])) # Basic unsorted with duplicate

    # Add more random examples if needed
    current_examples = len(examples)
    for _ in range(max(0, num_examples - current_examples)):
        size = random.randint(2, max_size)
        input_list = [random.randint(min_val, max_val) for _ in range(size)]
        output_list = sorted(input_list)
        examples.append((input_list, output_list))

    return examples[:num_examples] # Return exactly num_examples

def create_sort_prompt(examples: Optional[list[tuple[list[int], list[int]]]] = None) -> str:
    """
    Creates a prompt to ask an LLM for an efficient general-purpose sorting algorithm,
    optionally including examples.
    """
    base_prompt = (
        "Generate a Python function named `sort_algorithm` that implements an efficient sorting algorithm "
        "suitable for general use cases (handling various data distributions like random, sorted, reversed, duplicates, etc.).\n"
        "The function MUST take a list of numbers as input and return a new sorted list.\n"
        "IMPORTANT: The function MUST NOT use the built-in sorted() function or the .sort() method.\n"
        "IMPORTANT: The function MUST NOT print anything to standard output. It should ONLY return the sorted list."
    )

    if examples:
        example_str = "\n\nHere are some examples of how the function should behave:\n"
        for input_list, output_list in examples:
            example_str += f"Input: {input_list}\nOutput: {output_list}\n\n"
        return base_prompt + example_str.strip() # Add examples and remove trailing newline
    else:
        return base_prompt

# Test suite generation/loading functions are now in test_suite_generator.py

# --- Evaluation Logic ---

def evaluate_algorithm(generated_code: str, categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None) -> dict:
    """
    Evaluates the generated sorting algorithm code using provided test cases, optionally reporting progress.

    Args:
        generated_code: The Python code string generated by the LLM.
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates.
                           The dictionary passed to the callback might contain keys like:
                           'current_case', 'total_cases', 'category', 'status',
                           'input_snippet', 'output_snippet', 'error'.

    Returns:
        A dictionary containing evaluation results:
        - 'correctness': Overall correctness percentage.
        - 'avg_time_ms': Overall average LLM time (ms) for correct runs.
        - 'baseline_avg_time_ms': Overall average baseline time (ms).
        - 'performance_details': A dict mapping category names to {'correctness': float, 'avg_time_ms': float, 'baseline_avg_time_ms': float, 'count': int}.
        - 'error': Error message if execution or validation fails.
    """
    results = {
        'correctness': 0.0,
        'avg_time_ms': None,
        'baseline_avg_time_ms': None,
        'performance_details': {}, # To store per-category results
        'error': None
    }

    if not categorized_test_cases:
        results['error'] = "No test cases provided for evaluation."
        return results

    # Aggregation is now done inside the container. Initialize results structure.
    results = {
        'correctness': 0.0,
        'avg_time_ms': None,
        'baseline_avg_time_ms': None,
        'performance_details': {},
        'error': None
    }
    # Calculate total cases for initial progress reporting
    total_overall_cases_calculated = sum(len(cases) for cases in categorized_test_cases.values())

    # Define the Docker image to use
    DOCKER_IMAGE = "python:3.10-slim" # Or choose another appropriate Python image
    # Define container resource limits (adjust as needed)
    # EXEC_TIMEOUT is now handled by the wrapper script's logic or potentially docker exec timeout param
    EXEC_TIMEOUT_SECONDS = 300 # Timeout for the *entire* exec call (adjust as needed)
    CONTAINER_MEM_LIMIT = "1g" # Increased memory limit to 1 GB to handle large test cases
    CONTAINER_CPU_SHARES = 512 # Relative CPU weight (default 1024)

    # Initialize Docker client
    docker_client = None
    try:
        # Signal start of evaluation process
        if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Initializing', 'message': 'Initializing evaluation environment...'})

        docker_client = docker.from_env(timeout=10)
        docker_client.ping()
        print("Successfully connected to Docker daemon.")
        if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Connecting Docker', 'message': 'Docker client connected.'})
        # Ensure image exists
        try:
            docker_client.images.get(DOCKER_IMAGE)
            print(f"Docker image {DOCKER_IMAGE} found locally.")
            if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Checking Image', 'message': f'Docker image ready ({DOCKER_IMAGE}).'})
        except ImageNotFound:
            print(f"Pulling Docker image: {DOCKER_IMAGE}...")
            # Keep this message as pulling can take time
            if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Pulling Image', 'message': f'Pulling Docker image {DOCKER_IMAGE}...'})
            docker_client.images.pull(DOCKER_IMAGE)
            print(f"Docker image {DOCKER_IMAGE} pulled.")
            if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Image Pulled', 'message': f'Docker image pulled ({DOCKER_IMAGE}).'})
        print(f"Using Docker image: {DOCKER_IMAGE}")
    except (DockerConnectionError, APIError, Exception) as e:
        results['error'] = f"Docker initialization failed: {e}. Is Docker running?"
        print(f"Error: {results['error']}")
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results

    # Construct the absolute path to the Docker wrapper script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    wrapper_script_path = os.path.join(script_dir, DOCKER_WRAPPER_SCRIPT_NAME)

    # Read the Docker execution wrapper script content
    try:
        print(f"Attempting to read wrapper script from: {wrapper_script_path}") # Debug print
        with open(wrapper_script_path, 'r', encoding='utf-8') as f:
            exec_wrapper_code = f.read()
        print(f"Successfully read wrapper script: {DOCKER_WRAPPER_SCRIPT_NAME}") # Debug print
    except FileNotFoundError:
        results['error'] = f"Critical Error: Docker wrapper script '{DOCKER_WRAPPER_SCRIPT_NAME}' not found at expected path: {wrapper_script_path}."
        print(results['error'])
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results
    except Exception as e:
        results['error'] = f"Critical Error: Failed to read Docker wrapper script '{DOCKER_WRAPPER_SCRIPT_NAME}' from path '{wrapper_script_path}': {e}"
        print(results['error'])
        if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
        return results


    container = None
    # Use ExitStack for robust cleanup of tempdir and container
    with ExitStack() as stack:
        try:
           # Create TemporaryDirectory for the LLM code and runner script
           temp_dir = stack.enter_context(tempfile.TemporaryDirectory())
           llm_code_filename = "llm_sort.py"
           # Use the constant for the wrapper script filename on the host
           runner_script_filename_host = DOCKER_WRAPPER_SCRIPT_NAME
           llm_code_path_host = os.path.join(temp_dir, llm_code_filename)
           # The runner script is read from its fixed location, but we'll write its content
           # to the temp dir for copying into the container, using a consistent name inside.
           runner_script_filename_cont = "exec_runner.py" # Name inside container
           runner_script_path_host = os.path.join(temp_dir, runner_script_filename_cont) # Path on host temp dir
           # Define path for the test suite data file on the host
           test_suite_data_path_host = os.path.join(temp_dir, TEST_SUITE_DATA_FILENAME)

           sandbox_dir = "/sandbox" # Mount point inside container
           llm_code_path_cont = f"{sandbox_dir}/{llm_code_filename}"
           runner_script_path_cont = f"{sandbox_dir}/{runner_script_filename_cont}" # Container path for runner
           test_suite_data_path_cont = f"{sandbox_dir}/{TEST_SUITE_DATA_FILENAME}" # Container path for test data

           # --- DEBUG: Verify host files before container start ---
           print(f"DEBUG: Host temporary directory: {temp_dir}")
           print(f"DEBUG: Host LLM code path: {llm_code_path_host}")
           print(f"DEBUG: Host runner script path (in temp): {runner_script_path_host}")
           # --- END DEBUG ---

           # Write the generated code and the wrapper script (read earlier) to the host temp directory
           with open(llm_code_path_host, 'w', encoding='utf-8') as f_llm_script:
               f_llm_script.write(generated_code)
           with open(runner_script_path_host, 'w', encoding='utf-8') as f_runner_script:
               f_runner_script.write(exec_wrapper_code) # Write the content read from docker_exec_wrapper.py

           # --- DEBUG: Check if files exist on host before mount ---
           llm_exists = os.path.exists(llm_code_path_host)
           runner_exists = os.path.exists(runner_script_path_host)
           print(f"DEBUG: Host LLM code exists before run?: {llm_exists}")
           print(f"DEBUG: Host runner script exists before run?: {runner_exists}")
           if not llm_exists or not runner_exists:
                print("ERROR: Script files not found on host before starting container!")
                # Consider raising an error here if needed
           # --- END DEBUG ---

           # Write the test suite data to the host temp directory as JSON
           try:
               with open(test_suite_data_path_host, 'w', encoding='utf-8') as f_suite:
                   json.dump(categorized_test_cases, f_suite)
               print(f"DEBUG: Test suite data written to {test_suite_data_path_host}")
           except Exception as e:
               raise RuntimeError(f"Failed to write test suite data JSON to host: {e}")


           # Start the container once, keep it running
           print("Starting persistent Docker container...")
           container = docker_client.containers.run(
               image=DOCKER_IMAGE,
               command=["sleep", "infinity"], # Keep container alive
               # --- Volume mount removed, will copy files instead ---
               # volumes={temp_dir: {'bind': sandbox_dir}},
               working_dir=sandbox_dir, # Still useful for exec_run context
               mem_limit=CONTAINER_MEM_LIMIT,
               cpu_shares=CONTAINER_CPU_SHARES,
                detach=True,
                auto_remove=False, # We need to manage removal manually after loop
                # Security options
                # read_only=True, # Root filesystem read-only (volume is separate)
               # network_mode='none', # Disable networking
           )
           # Ensure container is stopped and removed at the end
           stack.callback(lambda c: (c.stop(timeout=5), c.remove(force=True)), container)
           print(f"Container {container.short_id} started.")
           if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Starting Container', 'message': f'Container started ({container.short_id}).'})

           # --- Create sandbox directory inside container ---
           # Although working_dir is set, explicitly create it for clarity and put_archive target
           print(f"Creating {sandbox_dir} inside container...")
           container.exec_run(cmd=f"mkdir -p {sandbox_dir}")

           # --- Prepare and copy files into the container ---
           print(f"Copying {llm_code_filename}, {runner_script_filename_cont}, and {TEST_SUITE_DATA_FILENAME} to container:{sandbox_dir}...")
           # Create an in-memory tar archive
           tar_stream = io.BytesIO()
           with tarfile.open(fileobj=tar_stream, mode='w') as tar:
               # Add llm_sort.py
               tar.add(llm_code_path_host, arcname=llm_code_filename)
               # Add the runner script using its container-internal name
               tar.add(runner_script_path_host, arcname=runner_script_filename_cont)
               # Add the test suite data file
               tar.add(test_suite_data_path_host, arcname=TEST_SUITE_DATA_FILENAME)

           tar_stream.seek(0) # Rewind the stream
           # Copy the archive to the container
           container.put_archive(path=sandbox_dir, data=tar_stream)
           print("Files copied.")
           if progress_callback: progress_callback({'status': 'Setup', 'category': 'Setup: Copying Files', 'message': 'Benchmark files copied to container.'})

           # Optional: Short delay after copy might still be beneficial? Unlikely needed now.
           # time.sleep(0.1)

           # --- Execute the wrapper script ONCE inside the container ---
           print(f"Executing wrapper script {runner_script_path_cont} in container...")
           # This is the final setup step before execution starts
           if progress_callback:
               progress_callback({
                   'status': 'Running', # Change status to Running here
                   'category': 'Executing All Cases', # Keep category general for the main run
                   'total_cases': total_overall_cases_calculated,
                   'message': 'Starting execution of all test cases in container...' # Updated message
               })

           exec_command = ["python", runner_script_path_cont]
           container_results = None
           exec_error = None

           try:
               # Use exec_run with stream=True, demux=True to get separate stdout/stderr
               exec_stream = container.exec_run(
                   cmd=exec_command,
                   stream=True,
                   demux=True, # Separate stdout (1) and stderr (2)
                   workdir=sandbox_dir,
               )

               stdout_acc = b""
               stderr_buffer = b"" # Buffer for potentially incomplete stderr lines
               exit_code = None # Will be determined after stream finishes

               print("Streaming output from container...")
               # Iterate safely, checking for None from the generator
               for item in exec_stream:
                   if item is None:
                       print("WARNING: Received None from exec_stream generator. Assuming stream ended.")
                       break # Exit the loop if None is received

                   # Now we know item is not None, proceed with unpacking
                   stream_type, chunk = item

                   # Still check if the chunk itself is None/empty, although less likely now
                   if chunk is None:
                       continue

                   if stream_type == 1: # stdout (final result)
                       stdout_acc += chunk
                   elif stream_type == 2: # stderr (progress updates)
                       stderr_buffer += chunk
                       # Process complete lines from stderr buffer
                       lines = stderr_buffer.split(b'\n')
                       stderr_buffer = lines[-1] # Keep incomplete line in buffer
                       for line_bytes in lines[:-1]:
                           line_str = line_bytes.decode('utf-8', errors='replace').strip()
                           if not line_str: continue # Skip empty lines
                           try:
                               progress_json = json.loads(line_str)
                               if progress_json.get("type") == "progress" and progress_callback:
                                   # Pass the inner 'data' dict to the callback
                                   progress_callback(progress_json.get("data", {}))
                               else:
                                   # Log unexpected JSON or non-progress messages from stderr
                                   print(f"DEBUG (stderr JSON): {line_str}")
                           except json.JSONDecodeError:
                               # Log non-JSON lines from stderr for debugging
                               print(f"DEBUG (stderr raw): {line_str}")
                           except Exception as cb_err:
                               print(f"ERROR: Progress callback failed: {cb_err}")

               # --- Stream finished, now inspect exit code and parse final result ---
               exec_inspect = docker_client.api.exec_inspect(exec_stream.exec_id) # Use exec_id from stream object
               exit_code = exec_inspect.get('ExitCode')

               # Handle potential None exit code (retry logic might be needed again if observed)
               if exit_code is None:
                   print(f"WARNING: exec_inspect returned ExitCode=None immediately after stream finished. Inspect: {exec_inspect}")
                   # Optionally add a small delay and retry inspect here if needed

               # Process final stdout result
               if exit_code == 0:
                   try:
                       output_str = stdout_acc.decode('utf-8').strip()
                       if not output_str:
                            raise ValueError("Container stdout was empty (Exit Code 0).")
                       # Expecting JSON like {"type": "result", "data": {...}}
                       final_message = json.loads(output_str)
                       if final_message.get("type") == "result":
                           container_results = final_message.get("data", {})
                           print("Successfully received and parsed final result from container stdout.")
                           results.update(container_results) # Update host results
                           if container_results.get('error'):
                               exec_error = f"Container wrapper script reported an internal error: {container_results['error']}"
                               results['error'] = exec_error # Ensure host result reflects this
                       else:
                           raise ValueError(f"Unexpected JSON format in stdout: Missing 'type' or not 'result'. Content: {output_str[:200]}...")
                   except (json.JSONDecodeError, ValueError) as json_e:
                       output_snippet = stdout_acc.decode('utf-8', errors='replace')[:1000]
                       exec_error = f"Failed to decode/parse final JSON result from container stdout (Exit Code 0): {json_e}. Output:\n---\n{output_snippet}\n---"
                       results['error'] = exec_error
                   except Exception as parse_e:
                       output_snippet = stdout_acc.decode('utf-8', errors='replace')[:1000]
                       exec_error = f"Unexpected error parsing container stdout (Exit Code 0): {parse_e}. Output:\n---\n{output_snippet}\n---"
                       results['error'] = exec_error
               elif exit_code is not None: # Execution failed (non-zero exit code)
                   stderr_snippet = stderr_buffer.decode('utf-8', errors='replace')[:500] # Include final stderr buffer content
                   stdout_snippet = stdout_acc.decode('utf-8', errors='replace')[:500]
                   exec_error = f"Container exec wrapper exited with code {exit_code}. Stderr:\n---\n{stderr_snippet}\n---\nStdout:\n---\n{stdout_snippet}\n---"
                   results['error'] = exec_error
               else: # Exit code remained None (problem inspecting exec)
                    exec_error = f"Failed to determine container exec exit code. Inspect result: {exec_inspect}"
                    results['error'] = exec_error


           except (APIError, DockerConnectionError) as docker_exec_err:
               exec_error = f"Docker API/Connection error during exec stream: {docker_exec_err}"
               results['error'] = exec_error
           except Exception as host_exec_e:
               exec_error = f"Host error during container exec_run call: {host_exec_e}"
               results['error'] = exec_error

           # --- Final Progress Update ---
           if progress_callback:
               final_status = 'Completed' if not results.get('error') else 'Error'
               progress_callback({
                   'status': final_status,
                   'category': 'Finished',
                   'total_cases': total_overall_cases_calculated,
                   'error': results.get('error'), # Report the final error status
                   'message': 'Container execution finished.'
               })

           if exec_error:
               print(f"Error during container execution: {exec_error}")
           # No more loops or per-case logic needed here

        # Catch errors during the initial container setup phase (remains the same)
        except (APIError, DockerConnectionError) as docker_setup_err:
            llm_error_str = f"Docker API/Connection error during container setup: {docker_setup_err}"
            results['error'] = llm_error_str
            print(f"Aborting evaluation due to Docker setup error: {llm_error_str}")
            if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
            # ExitStack will handle cleanup if container was partially started
            return results
        except Exception as setup_exec_e:
            llm_error_str = f"Error setting up container or temporary directory: {setup_exec_e}"
            results['error'] = llm_error_str
            print(f"Aborting evaluation due to setup error: {llm_error_str}")
            if progress_callback: progress_callback({'status': 'Error', 'error': results['error']})
            # ExitStack will handle cleanup
            return results
        finally:
            # ExitStack automatically handles stopping/removing the container and deleting the temp dir
            if container:
                 print(f"Container {container.short_id} stopped and removed.")
            else:
                 print("No container was started or setup failed.")


    # --- Final Results ---
    # The 'results' dictionary is now populated directly from the container's output
    # or contains an error message if the execution failed.
    # No further calculation needed here.

    return results


# Note: algorithm_name parameter is removed. We use the constant GENERAL_ALGORITHM_NAME internally.
# Now accepts pre-generated code.
def run_single_benchmark(llm_name: str, generated_code: str, categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None) -> dict:
    """
    Runs the evaluation part of a benchmark for a single LLM, using pre-generated code
    and provided test cases, optionally reporting progress.

    Args:
        llm_name: The identifier for the LLM used (for reporting).
        generated_code: The Python code string generated by the LLM.
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates during evaluation.

    Returns:
        A dictionary containing benchmark evaluation results.
    """
    algorithm_name = GENERAL_ALGORITHM_NAME # Use the constant label
    print(f"Evaluating benchmark for {llm_name} ({algorithm_name}) using provided code...")

    # Code generation is now done *before* calling this function.
    # The initial progress callback indicating evaluation start is also handled before this call.

    # --- Run Evaluation ---
    evaluation = evaluate_algorithm(
        generated_code=generated_code, # Use the passed-in code
        categorized_test_cases=categorized_test_cases,
        progress_callback=progress_callback # Pass callback for evaluation steps
    )

    # --- Callback for completion of evaluation ---
    final_status = 'Completed' if not evaluation.get('error') else 'Error'
    if progress_callback:
        progress_callback({
            'status': final_status, 'category': 'Finished',
            'error': evaluation.get('error')
        })

    return {
        'llm': llm_name,
        'algorithm': algorithm_name,
        'correctness': evaluation.get('correctness'),
        'avg_time_ms': evaluation.get('avg_time_ms'),
        'baseline_avg_time_ms': evaluation.get('baseline_avg_time_ms'),
        'performance_details': evaluation.get('performance_details'),
        'error': evaluation.get('error'),
        # 'generated_code' is no longer added here, it's handled in app.py
    }

# Note: algorithm_name parameter removed, using constant label BASELINE_ALGORITHM_NAME instead.
def run_python_sorted_benchmark(categorized_test_cases: dict, progress_callback: Optional[Callable[[dict], None]] = None, python_sorted_identifier: str = "Python sorted()") -> dict:
    """
    Runs a benchmark using Python's built-in sorted() function as a baseline,
    using provided test cases and optionally reporting progress.

    Args:
        categorized_test_cases: A dictionary containing the test cases, keyed by category.
        progress_callback: An optional function to call with progress updates.

    Returns:
        A dictionary containing benchmark results.
    """
    algorithm_name = BASELINE_ALGORITHM_NAME # Use the constant label
    print(f"Running {algorithm_name} benchmark...")
    results = {
        'llm': python_sorted_identifier, # Use the passed identifier (e.g., from app.py)
        'algorithm': algorithm_name, # Use the constant label
        'correctness': 100.0, # sorted() is assumed correct
        'avg_time_ms': None, # This is the baseline itself
        'baseline_avg_time_ms': None, # Will be calculated
        'performance_details': {}, # To store per-category results
        'error': None,
        'generated_code': "N/A - Python sorted()"
    }

    if not categorized_test_cases:
        results['error'] = "No test cases provided for baseline benchmark."
        return results

    # Aggregators for overall results
    overall_baseline_time = 0
    overall_total_cases = 0

    # Aggregators for per-category results
    category_results = defaultdict(lambda: {'baseline_time': 0, 'case_count': 0})

    try:
        # Calculate total cases for progress reporting
        total_overall_cases_calculated = sum(len(cases) for cases in categorized_test_cases.values())
        current_overall_case_num = 0

        # Iterate through each category and its test cases
        for category, test_cases_in_category in categorized_test_cases.items():
            print(f"  Benchmarking category: {category} ({len(test_cases_in_category)} cases)")
            cat_stats = category_results[category] # Get stats dict for this category
            num_cases_in_category = len(test_cases_in_category)

            for i, test_case in enumerate(test_cases_in_category):
                current_overall_case_num += 1
                overall_total_cases += 1 # Increment the overall counter
                cat_stats['case_count'] += 1 # Increment here

                # --- Prepare data for callback ---
                progress_data = {
                    'current_case': current_overall_case_num,
                    'total_cases': total_overall_cases_calculated,
                    'category': category,
                   'category_case_num': i + 1,
                   'category_total_cases': num_cases_in_category,
                   'status': 'Running',
                   'input_snippet': repr(test_case[:10]) + ('...' if len(test_case) > 10 else ''),
                   # Calculate the expected output snippet for baseline
                   'output_snippet': repr(sorted(test_case)[:10]) + ('...' if len(test_case) > 10 else ''),
                   'error': None
               }
               # Report start of case processing (optional for baseline, but consistent)
                # if progress_callback: progress_callback(progress_data)

                baseline_input = list(test_case)
                start_time = time.perf_counter()
                _ = sorted(baseline_input) # Execute and time sorted()
                end_time = time.perf_counter()
                current_baseline_time = end_time - start_time

                overall_baseline_time += current_baseline_time
                cat_stats['baseline_time'] += current_baseline_time

                # Report completion of case for baseline
                if progress_callback:
                    progress_data['status'] = 'Correct (Baseline)' # Baseline is assumed correct
                    progress_callback(progress_data)


        # --- Calculate Final Results ---

        # Overall results
        if overall_total_cases > 0:
            overall_avg_time = (overall_baseline_time / overall_total_cases) * 1000
            # Since this *is* the baseline, set both avg_time and baseline_avg_time
            results['avg_time_ms'] = overall_avg_time
            results['baseline_avg_time_ms'] = overall_avg_time
        else:
             results['avg_time_ms'] = 0.0
             results['baseline_avg_time_ms'] = 0.0

        # Per-category results
        for category, stats in category_results.items():
           cat_avg_baseline_time = None
           if stats['case_count'] > 0:
               cat_avg_baseline_time = (stats['baseline_time'] / stats['case_count']) * 1000
           results['performance_details'][category] = {
               'correctness': 100.0, # Assumed correct
               'avg_time_ms': cat_avg_baseline_time, # This is the baseline time for this category
                'baseline_avg_time_ms': cat_avg_baseline_time,
                'count': stats['case_count']
            }

    except Exception as e:
        results['error'] = f"Error during Python sorted() execution: {e}"
        results['correctness'] = None # Mark correctness as unknown if execution fails
        # Ensure performance_details is still present, even if empty
        results['performance_details'] = results.get('performance_details', {})

    return results


# Note: The __main__ block below is for standalone testing/example usage of benchmark.py.
# Test suite generation is now handled by test_suite_generator.py.

if __name__ == '__main__':
    # Import argparse here as it's only needed for CLI execution
    import argparse
    # Import the constant needed for the example run, if running standalone
    # Define a local constant for the example run if app isn't available
    LOCAL_PYTHON_SORTED_BENCHMARK_ID = "Python sorted() [Standalone]"

    parser = argparse.ArgumentParser(description="Benchmark Execution Examples")
    # Use the default from the generator module
    parser.add_argument('--suite-file', default=test_suite_generator.DEFAULT_TEST_SUITE_FILE, help="Specify the test suite JSON file path")

    args = parser.parse_args()

    # Example of running benchmarks using a loaded suite
    print("Running example benchmarks with loaded suite...")
    try:
        # Load the test suite using the function from the generator module
        test_suite = test_suite_generator.load_test_suite(args.suite_file)

        # Example usage - Load suite first, then run benchmarks
        print("\nRunning example benchmarks with loaded suite...")

        # Run baseline benchmark
        print("\nRunning baseline benchmark example...")
        # Pass the local identifier for standalone runs
        result_baseline = run_python_sorted_benchmark(
            categorized_test_cases=test_suite,
            python_sorted_identifier=LOCAL_PYTHON_SORTED_BENCHMARK_ID
        )
        print("\nPython sorted() Benchmark Result:\n", json.dumps(result_baseline, indent=2))

        # --- Run example with fixed Merge Sort code ---
        print("\nRunning example benchmark with fixed Merge Sort code...")

        EXAMPLE_MERGE_SORT_CODE = """
import sys
from typing import List, TypeVar

# Increase recursion depth limit for potentially deep recursion with large lists
try:
    sys.setrecursionlimit(2000)
except Exception:
    pass

T = TypeVar('T')

def sort_algorithm(arr: List[T]) -> List[T]:
    # Base Case
    if len(arr) <= 1:
        return arr[:]

    # Divide
    mid = len(arr) // 2
    left_half = arr[:mid]
    right_half = arr[mid:]

    # Conquer
    sorted_left = sort_algorithm(left_half)
    sorted_right = sort_algorithm(right_half)

    # Combine (Merge)
    merged = []
    left_idx, right_idx = 0, 0
    while left_idx < len(sorted_left) and right_idx < len(sorted_right):
        if sorted_left[left_idx] <= sorted_right[right_idx]:
            merged.append(sorted_left[left_idx])
            left_idx += 1
        else:
            merged.append(sorted_right[right_idx])
            right_idx += 1

    # Append Remaining
    while left_idx < len(sorted_left):
        merged.append(sorted_left[left_idx])
        left_idx += 1
    while right_idx < len(sorted_right):
        merged.append(sorted_right[right_idx])
        right_idx += 1

    return merged
"""
            # Run the benchmark using the fixed code
        result_example = run_single_benchmark(
                llm_name="Example Merge Sort", # Use a descriptive name
                generated_code=EXAMPLE_MERGE_SORT_CODE,
                categorized_test_cases=test_suite
                # Add progress_callback=print if you want to see progress updates
            )
        print("\nExample Merge Sort Benchmark Result:\n", json.dumps(result_example, indent=2))


    except FileNotFoundError:
            print(f"Test suite file '{args.suite_file}' not found. Generate it first using: python test_suite_generator.py --generate-suite")
    except Exception as e:
            print(f"An error occurred during example benchmark run: {e}")
