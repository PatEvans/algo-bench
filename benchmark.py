"""
Module for benchmarking LLM-generated sorting algorithms.

Handles:
- Generating prompts for LLMs.
- Safely executing the generated code.
- Evaluating correctness and performance.
"""

import time
import random
import llm_interface
# Need a secure way to execute code, e.g., using restricted environments or sandboxing.
# This is a critical security consideration. A simple `exec` is DANGEROUS.

def create_sort_prompt(algorithm_name: str) -> str:
    """Creates a prompt to ask an LLM for a specific sorting algorithm."""
    return f"Generate a Python function named `sort_algorithm` that implements {algorithm_name}. The function should take a list of numbers as input and return a new sorted list."

def generate_test_cases() -> list[list[int]]:
    """Generates a list of test cases for sorting algorithms."""
    return [
        [],
        [1],
        [1, 2, 3, 4, 5],
        [5, 4, 3, 2, 1],
        [3, 1, 4, 1, 5, 9, 2, 6],
        [random.randint(0, 1000) for _ in range(100)], # Larger random list
        [-5, 0, -10, 5, -1],
        [2, 2, 1, 1, 3, 3], # Duplicates
    ]

def evaluate_algorithm(generated_code: str) -> dict:
    """
    Evaluates the generated sorting algorithm code.

    Args:
        generated_code: The Python code string generated by the LLM.

    Returns:
        A dictionary containing evaluation results (correctness, execution_time).
        Returns {'error': 'message'} if execution or validation fails.
    """
    results = {'correctness': 0.0, 'avg_time_ms': None, 'error': None}
    test_cases = generate_test_cases()
    correct_count = 0
    total_time = 0

    try:
        # !!! SECURITY WARNING !!!
        # Executing arbitrary code from LLMs is highly risky.
        # `exec` is used here for simplicity but is NOT SAFE for production.
        # A proper sandboxing mechanism (e.g., Docker containers, restricted Python interpreters)
        # is essential to prevent malicious code execution.
        local_namespace = {}
        exec(generated_code, {}, local_namespace)

        if 'sort_algorithm' not in local_namespace or not callable(local_namespace['sort_algorithm']):
            results['error'] = "Generated code does not contain a callable function named 'sort_algorithm'."
            return results

        sort_func = local_namespace['sort_algorithm']

        for test_case in test_cases:
            test_input = list(test_case) # Ensure original is not modified
            expected_output = sorted(test_case)

            start_time = time.perf_counter()
            try:
                actual_output = sort_func(test_input)
                end_time = time.perf_counter()

                if actual_output == expected_output:
                    correct_count += 1
                    total_time += (end_time - start_time)
                else:
                    print(f"Incorrect sort: Input={test_case}, Expected={expected_output}, Got={actual_output}")
            except Exception as e:
                print(f"Error during sort execution: Input={test_case}, Error={e}")
                # Optionally penalize for runtime errors

        if test_cases:
            results['correctness'] = (correct_count / len(test_cases)) * 100
        if correct_count > 0:
             # Average time only for correctly sorted cases
            results['avg_time_ms'] = (total_time / correct_count) * 1000

    except SyntaxError as e:
        results['error'] = f"Syntax error in generated code: {e}"
    except Exception as e:
        results['error'] = f"Error executing generated code: {e}"

    return results


def run_single_benchmark(llm_name: str, algorithm_name: str) -> dict:
    """Runs a benchmark for a single LLM and algorithm."""
    print(f"Running benchmark for {llm_name} on {algorithm_name}...")
    prompt = create_sort_prompt(algorithm_name)
    generated_code = llm_interface.generate_code(llm_name, prompt)

    if not generated_code:
        return {'llm': llm_name, 'algorithm': algorithm_name, 'error': 'Failed to generate code'}

    evaluation = evaluate_algorithm(generated_code)

    return {
        'llm': llm_name,
        'algorithm': algorithm_name,
        'correctness': evaluation.get('correctness'),
        'avg_time_ms': evaluation.get('avg_time_ms'),
        'error': evaluation.get('error'),
        # 'generated_code': generated_code # Optionally store the code
    }

# Example usage
if __name__ == '__main__':
    result = run_single_benchmark('dummy_llm', 'Bubble Sort')
    print("\nBenchmark Result:\n", result)

    result_quick = run_single_benchmark('dummy_llm', 'Quick Sort')
    print("\nBenchmark Result:\n", result_quick)
