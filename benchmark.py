"""
Module for benchmarking LLM-generated sorting algorithms.

Handles:
- Generating prompts for LLMs.
- Safely executing the generated code.
- Evaluating correctness and performance.
"""

import time
import random
import llm_interface
# Need a secure way to execute code, e.g., using restricted environments or sandboxing.
# This is a critical security consideration. A simple `exec` is DANGEROUS.

def create_sort_prompt(algorithm_name: str) -> str:
    """Creates a prompt to ask an LLM for a specific sorting algorithm."""
    return f"Generate a Python function named `sort_algorithm` that implements {algorithm_name}. The function should take a list of numbers as input and return a new sorted list."

def generate_test_cases(size_small=10, size_medium=1000, size_large=100000, num_cases_per_type=2) -> list[list[int]]:
    """
    Generates integer test cases based on specified patterns:
    - Randomized (within a range)
    - Duplicates (many repeating elements)
    - Sorted (ascending)
    - Reversed (descending)

    Args:
        size_small: Size for small test cases.
        size_medium: Size for medium test cases.
        size_large: Size for large test cases.
                    (Note: Very large sizes like 10M might exceed memory/time limits).
        num_cases_per_type: Number of random/duplicate cases to generate per size.

    Returns:
        A list of lists, where each inner list is an integer test case.
    """
    cases = [
        [],    # Empty list
        [5],   # Single element
    ]
    sizes = {'small': size_small, 'medium': size_medium, 'large': size_large}
    # Define range for random numbers, e.g., based on size
    min_val, max_val = -10000, 10000

    for name, size in sizes.items():
        if size == 0: continue # Skip size 0 if specified

        print(f"Generating cases for size: {name} ({size})...")

        # 1. Randomized
        for i in range(num_cases_per_type):
            random_case = [random.randint(min_val, max_val) for _ in range(size)]
            cases.append(random_case)
            print(f"  - Added random case {i+1}")

        # 2. With Duplicates (e.g., numbers from a smaller range)
        duplicate_range_max = max(1, size // 10) # Ensure range is at least 1
        for i in range(num_cases_per_type):
            duplicate_case = [random.randint(0, duplicate_range_max) for _ in range(size)]
            cases.append(duplicate_case)
            print(f"  - Added duplicate case {i+1}")

        # 3. Sorted (Ascending)
        sorted_case = list(range(size))
        cases.append(sorted_case)
        print(f"  - Added sorted case")

        # 4. Reversed (Descending)
        reversed_case = list(range(size, 0, -1))
        cases.append(reversed_case)
        print(f"  - Added reversed case")

    # Example of adding a very large case (use with caution!)
    # size_xl = 10_000_000
    # print(f"Generating XL random case ({size_xl})...")
    # cases.append([random.randint(min_val, max_val) for _ in range(size_xl)])

    print(f"Generated a total of {len(cases)} test cases.")
    return cases

def evaluate_algorithm(generated_code: str) -> dict:
    """
    Evaluates the generated sorting algorithm code.

    Args:
        generated_code: The Python code string generated by the LLM.

    Returns:
        A dictionary containing evaluation results (correctness, avg_time_ms, baseline_avg_time_ms).
        Returns {'error': 'message'} if execution or validation fails.
    """
    results = {'correctness': 0.0, 'avg_time_ms': None, 'baseline_avg_time_ms': None, 'error': None}
    try:
        test_cases = generate_test_cases()
    except Exception as e:
        results['error'] = f"Failed to generate test cases: {e}"
        return results

    correct_count = 0
    total_llm_time = 0
    total_baseline_time = 0

    try:
        # !!! SECURITY WARNING !!!
        # Executing arbitrary code from LLMs is highly risky.
        # `exec` is used here for simplicity but is NOT SAFE for production.
        # A proper sandboxing mechanism (e.g., Docker containers, restricted Python interpreters)
        # is essential to prevent malicious code execution.
        local_namespace = {}
        exec(generated_code, {}, local_namespace)

        if 'sort_algorithm' not in local_namespace or not callable(local_namespace['sort_algorithm']):
            results['error'] = "Generated code does not contain a callable function named 'sort_algorithm'."
            return results

        sort_func = local_namespace['sort_algorithm']

        for test_case in test_cases:
             # Prepare inputs for both sorts
            llm_input = list(test_case) # Ensure original is not modified for LLM
            baseline_input = list(test_case) # Ensure original is not modified for baseline
            expected_output = sorted(test_case) # Ground truth

            # --- Time LLM's sort_algorithm ---
            llm_start_time = time.perf_counter()
            actual_output = None
            llm_error = None
            try:
                actual_output = sort_func(llm_input)
                llm_end_time = time.perf_counter()
                current_llm_time = llm_end_time - llm_start_time

                if actual_output == expected_output:
                    correct_count += 1
                    total_llm_time += current_llm_time # Only sum time for correct sorts
                else:
                    # Truncate output for printing if it's a list and long
                    actual_repr = repr(actual_output[:20]) + '...' if isinstance(actual_output, list) and len(actual_output) > 20 else repr(actual_output)
                    expected_repr = repr(expected_output[:20]) + '...' if len(expected_output) > 20 else repr(expected_output)
                    test_repr = repr(test_case[:20]) + '...' if len(test_case) > 20 else repr(test_case)
                    print(f"Incorrect sort: Input={test_repr}, Expected={expected_repr}, Got={actual_repr}")

            except Exception as e:
                llm_end_time = time.perf_counter() # Still record time until failure if possible
                llm_error = e
                test_repr = repr(test_case[:20]) + '...' if len(test_case) > 20 else repr(test_case)
                print(f"Error during LLM sort execution: Input={test_repr}, Error={e}")
                # Do not add time to total_llm_time if it errored

            # --- Time Python's built-in sorted() ---
            baseline_start_time = time.perf_counter()
            try:
                _ = sorted(baseline_input) # Execute baseline sort
                baseline_end_time = time.perf_counter()
                total_baseline_time += (baseline_end_time - baseline_start_time)
            except Exception as e:
                # This should ideally not happen with built-in sorted
                test_repr = repr(test_case[:20]) + '...' if len(test_case) > 20 else repr(test_case)
                print(f"Error during baseline sort execution: Input={test_repr}, Error={e}")
                # If baseline fails, we might not want to record its time? Or record as infinity? For now, just print.


        if test_cases:
            results['correctness'] = (correct_count / len(test_cases)) * 100 if len(test_cases) > 0 else 0.0
        if correct_count > 0:
             # Average LLM time only over correctly sorted cases
            results['avg_time_ms'] = (total_llm_time / correct_count) * 1000
        if len(test_cases) > 0: # Average baseline time over all cases attempted
             results['baseline_avg_time_ms'] = (total_baseline_time / len(test_cases)) * 1000

    except SyntaxError as e:
        results['error'] = f"Syntax error in generated code: {e}"
    except Exception as e:
        results['error'] = f"Error executing generated code: {e}"

    return results


def run_single_benchmark(llm_name: str, algorithm_name: str) -> dict:
    """Runs a benchmark for a single LLM and algorithm."""
    print(f"Running benchmark for {llm_name} on {algorithm_name}...")
    prompt = create_sort_prompt(algorithm_name)
    generated_code = llm_interface.generate_code(llm_name, prompt)

    if not generated_code:
        return {'llm': llm_name, 'algorithm': algorithm_name, 'error': 'Failed to generate code'}

    evaluation = evaluate_algorithm(generated_code)

    return {
        'llm': llm_name,
        'algorithm': algorithm_name,
        'correctness': evaluation.get('correctness'),
        'avg_time_ms': evaluation.get('avg_time_ms'),
        'baseline_avg_time_ms': evaluation.get('baseline_avg_time_ms'),
        'error': evaluation.get('error'),
        'generated_code': generated_code # Optionally store the code
    }


def run_python_sorted_benchmark(algorithm_name: str) -> dict:
    """
    Runs a benchmark using Python's built-in sorted() function.

    Args:
        algorithm_name: The conceptual algorithm name (used for labeling, not execution).

    Returns:
        A dictionary containing benchmark results.
    """
    print(f"Running Python sorted() benchmark (labeled as {algorithm_name})...")
    results = {
        'llm': PYTHON_SORTED_BENCHMARK, # Use the constant defined in app.py or define locally
        'algorithm': algorithm_name,
        'correctness': 100.0, # sorted() is assumed correct
        'avg_time_ms': None,
        'baseline_avg_time_ms': None, # Baseline is itself in this case
        'error': None,
        'generated_code': "N/A - Python sorted()"
    }

    try:
        test_cases = generate_test_cases()
    except Exception as e:
        results['error'] = f"Failed to generate test cases: {e}"
        return results

    if not test_cases:
        results['error'] = "No test cases generated."
        return results

    total_time = 0
    try:
        for test_case in test_cases:
            baseline_input = list(test_case) # Ensure original is not modified
            start_time = time.perf_counter()
            _ = sorted(baseline_input) # Execute and time sorted()
            end_time = time.perf_counter()
            total_time += (end_time - start_time)

        avg_time = (total_time / len(test_cases)) * 1000
        results['avg_time_ms'] = avg_time
        # Since this *is* the baseline, set baseline time to the same value
        results['baseline_avg_time_ms'] = avg_time

    except Exception as e:
        results['error'] = f"Error during Python sorted() execution: {e}"
        results['correctness'] = None # Mark correctness as unknown if execution fails

    return results


# Example usage needs the constant defined in app.py or locally
PYTHON_SORTED_BENCHMARK = "Python sorted()" # Define locally for example usage

if __name__ == '__main__':
    result_llm = run_single_benchmark('dummy_llm', 'Bubble Sort')
    print("\nLLM Benchmark Result:\n", result_llm)

    result_baseline = run_python_sorted_benchmark('Bubble Sort') # Label baseline run
    print("\nPython sorted() Benchmark Result:\n", result_baseline)

    # result_quick = run_single_benchmark('dummy_llm', 'Quick Sort')
    # print("\nBenchmark Result:\n", result_quick)
